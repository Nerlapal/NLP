# NLP
# BERT Finetuning for Multilingual Subjective Classification
This repository contains code that utilizes PyTorch and Adam Optimizer to finetune the pre-trained multilingual BERT model from Hugging Face for subjective classification tasks. The dataset used for the task consists of news articles that were augmented. The model was fine-tuned on 15 labels in 12 different languages, and the Hugging Face transformers framework was used.


To perform the fine-tuning process, one can run the BERT_Finetune.ipynb notebook in Jupyter Notebook or JupyterLab. The notebook provides detailed instructions on how to prepare the data, fine-tune the model, and evaluate its performance. The model exhibited good accuracy in subjective label classification.
